{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Packages\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os \n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.applications import VGG16\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "annotations_file = 'path\\to\\annotations'\n",
    "with open(annotations_file, 'r') as file:\n",
    "    annotations = json.load(file)\n",
    "\n",
    "# Parse annotations\n",
    "data_directory = r'\\path\\to\\directory'\n",
    "data = []\n",
    "for filename, file_data in annotations.items():\n",
    "\n",
    "    image_path = data_directory+'\\\\'+filename.split('.')[0]+'.png'\n",
    "    image = Image.open(image_path)\n",
    "    for region in file_data['regions']:\n",
    "\n",
    "        shape_attrs = region['shape_attributes']\n",
    "        region_attrs = region['region_attributes']\n",
    "        temp = {\n",
    "            'filename': filename.split('.')[0]+'.png',\n",
    "            'name': shape_attrs['name'],\n",
    "            'width': image.width,\n",
    "            'height': image.height,\n",
    "            'all_points_x': shape_attrs['all_points_x'],\n",
    "            'all_points_y': shape_attrs['all_points_y'],\n",
    "            'Type': region_attrs['Type']\n",
    "        }\n",
    "\n",
    "        #Don't include duplicates\n",
    "        if temp not in data:\n",
    "            data.append({\n",
    "                'filename': filename.split('.')[0]+'.png',\n",
    "                'name': shape_attrs['name'],\n",
    "                'width': image.width,\n",
    "                'height': image.height,\n",
    "                'all_points_x': shape_attrs['all_points_x'],\n",
    "                'all_points_y': shape_attrs['all_points_y'],\n",
    "                'Type': region_attrs['Type']\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_regions(data, images_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for img in data:\n",
    "        \n",
    "        filename = img['filename']\n",
    "        image_path = os.path.join(images_dir, filename)\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Image {filename} not found.\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        label = img['Type']\n",
    "        all_points_x = img['all_points_x']\n",
    "        all_points_y = img['all_points_y']\n",
    "        points = np.array(list(zip(all_points_x, all_points_y)), dtype=np.int32)\n",
    "\n",
    "        # Create a mask\n",
    "        mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "        cv2.fillPoly(mask, [points], 255)\n",
    "\n",
    "        # Extract the region using the mask\n",
    "        masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "        # Find bounding box and crop region\n",
    "        x, y, w, h = cv2.boundingRect(points)\n",
    "        cropped_image = masked_image[y:y+h, x:x+w]\n",
    "        \n",
    "        # Save the extracted region\n",
    "        label_dir = os.path.join(output_dir, label)\n",
    "        if not os.path.exists(label_dir):\n",
    "            os.makedirs(label_dir)\n",
    "\n",
    "        roi_filename = os.path.join(label_dir, f\"{filename}_{x}_{y}.png\")\n",
    "        cv2.imwrite(roi_filename, cropped_image)\n",
    "        print(f\"Saved ROI to {roi_filename}\")\n",
    "\n",
    "# Directory paths\n",
    "images_dir = 'path\\to\\directory'\n",
    "output_dir = 'path\\to\\output'\n",
    "\n",
    "extract_regions(data, images_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define paths\n",
    "dataset_dir = 'path\\to\\dataset'\n",
    "output_dir = 'path\\to\\train_test_val'\n",
    "train_dir = os.path.join(output_dir, 'train')\n",
    "val_dir = os.path.join(output_dir, 'val')\n",
    "test_dir = os.path.join(output_dir, 'test')\n",
    "\n",
    "# Create output directories if they do not exist\n",
    "for dir_path in [train_dir, val_dir, test_dir]:\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "# Function to split and copy data\n",
    "def split_and_copy_data(class_dir, train_dir, val_dir, test_dir, val_size=0.2, test_size=0.1):\n",
    "    # Get list of all images in the class directory\n",
    "    images = [f for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))]\n",
    "    \n",
    "    # Split data\n",
    "    train_images, temp_images = train_test_split(images, test_size=(val_size + test_size))\n",
    "    val_images, test_images = train_test_split(temp_images, test_size=test_size / (val_size + test_size))\n",
    "\n",
    "    # Define class name\n",
    "    class_name = os.path.basename(class_dir)\n",
    "    \n",
    "    # Create class directories in train, val, test directories\n",
    "    for split_dir in [train_dir, val_dir, test_dir]:\n",
    "        class_split_dir = os.path.join(split_dir, class_name)\n",
    "        if not os.path.exists(class_split_dir):\n",
    "            os.makedirs(class_split_dir)\n",
    "    \n",
    "    # Copy images to respective directories\n",
    "    for image in train_images:\n",
    "        shutil.copy(os.path.join(class_dir, image), os.path.join(train_dir, class_name, image))\n",
    "    for image in val_images:\n",
    "        shutil.copy(os.path.join(class_dir, image), os.path.join(val_dir, class_name, image))\n",
    "    for image in test_images:\n",
    "        shutil.copy(os.path.join(class_dir, image), os.path.join(test_dir, class_name, image))\n",
    "\n",
    "# Iterate over each class directory and split data\n",
    "for class_name in os.listdir(dataset_dir):\n",
    "    class_dir = os.path.join(dataset_dir, class_name)\n",
    "    if os.path.isdir(class_dir):\n",
    "        split_and_copy_data(class_dir, train_dir, val_dir, test_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model (VGG 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Directories\n",
    "train_dir = 'path\\to\\train_test_val\\train\\'\n",
    "validation_dir = 'path\\to\\train_test_val\\val\\'\n",
    "\n",
    "# Image Data Generator with Data Augmentation\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#Create generators (adjust size if needed)\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "validation_generator = validation_datagen.flow_from_directory(validation_dir, target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# Model Building\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "history = model.fit(train_generator, epochs=50, validation_data=validation_generator,  callbacks=[reduce_lr, early_stopping])\n",
    "\n",
    "# Save the model\n",
    "model.save('model_name.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'path\\to\\train_test_val\\test\\'\n",
    "\n",
    "# Create an ImageDataGenerator for the test set (no augmentation needed for testing) \n",
    "test_datagen = ImageDataGenerator(rescale=0.255)\n",
    "# Define sizes (adjust if needed)\n",
    "image_height=64\n",
    "batch_size=32\n",
    "image_width=64\n",
    "# Create a test generator\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(image_height, image_width),  # Specify the same size as used during training\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical', \n",
    "    shuffle=False \n",
    ")\n",
    "# Predict the class probabilities for the test set\n",
    "predictions = model.predict(test_generator, steps=test_generator.samples // batch_size + 1)\n",
    "\n",
    "# Get the true and predicted labels\n",
    "true_labels = test_generator.classes\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(true_labels, predicted_labels, target_names=test_generator.class_indices.keys()))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Other Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model_name.keras'\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'path\\to\\train_test_val\\test\\'\n",
    "\n",
    "# Create an ImageDataGenerator for the test set (no augmentation needed for testing)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "# Create a test generator (adjust size if needed)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(64,64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "class_labels = {v: k for k, v in test_generator.class_indices.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'path\\to\\images'\n",
    "results = []\n",
    "for img_name in os.listdir(image_folder):\n",
    "    img_path = os.path.join(image_folder, img_name)\n",
    "    if os.path.isfile(img_path):\n",
    "        # Load and preprocess the image\n",
    "        img_array = load_and_preprocess_image(img_path, target_size)\n",
    "        \n",
    "        # Predict the class probabilities\n",
    "        predictions = model.predict(img_array)\n",
    "        \n",
    "        # Get the predicted class index\n",
    "        predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
    "        \n",
    "        # Get the predicted class label\n",
    "        predicted_class_label = class_labels[predicted_class_index]\n",
    "        \n",
    "        # Store the result\n",
    "        results.append((img_name, predicted_class_label))\n",
    "\n",
    "        print(f\"Image: {img_name}, Predicted class: {predicted_class_label}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
